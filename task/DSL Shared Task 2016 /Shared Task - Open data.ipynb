{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import codecs\n",
    "from collections import Counter\n",
    "import random \n",
    "import numpy as np\n",
    "from numpy.random import permutation, shuffle, rand\n",
    "from numpy.linalg import svd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def from_buck_to_utf8(text):\n",
    "    b2a = {'A': u'\\u0627',  '<': u'\\u0625',  '|': u'\\u0622',  '>': u'\\u0623',  \"'\": u'\\u0621',  'b': u'\\u0628',  \n",
    "           't': u'\\u062a',  'v': u'\\u062b',  'j': u'\\u062c',  'H': u'\\u062d',  'x': u'\\u062e',  'd': u'\\u062f',  \n",
    "           '*': u'\\u0630',  'r': u'\\u0631',  'z': u'\\u0632',  's': u'\\u0633',  '$': u'\\u0634',  'S': u'\\u0635',  \n",
    "           'D': u'\\u0636',  'T': u'\\u0637',  'Z': u'\\u0638',  'E': u'\\u0639',  'g': u'\\u063a',  'f': u'\\u0641',  \n",
    "           'q': u'\\u0642',  'k': u'\\u0643',  'l': u'\\u0644',  'm': u'\\u0645',  'n': u'\\u0646',  'h': u'\\u0647',  \n",
    "           'w': u'\\u0648',  'y': u'\\u064a',  'Y': u'\\u0649',  'p': u'\\u0629',  '&': u'\\u0624',  '}': u'\\u0626',  \n",
    "           'a': u'\\u064e',  'F': u'\\u064b',  'u': u'\\u064f',  'N': u'\\u064c',  'i': u'\\u0650',  'K': u'\\u064d',  \n",
    "           'o': u'\\u0652',  '~': u'\\u0651'}\n",
    "    text = text.strip().split()\n",
    "    tmp_sentence = list()\n",
    "    for word in text:\n",
    "        tmp_word = list()\n",
    "        for c in word:\n",
    "            tmp_word.append(b2a.get(c,c))\n",
    "        else:\n",
    "            tmp_sentence.append(''.join(tmp_word))\n",
    "    else:\n",
    "        return ' '.join(tmp_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "egyptian\t\n",
      "\n",
      "egyptian\t\n",
      "\n",
      "8925\n",
      "10978\n",
      "maghrebi\t\n",
      "\n",
      "6939\n",
      "7039\n",
      "levantine\t\n",
      "\n",
      "4810\n",
      "6405\n",
      "gulf\t\n",
      "\n",
      "6306\n",
      "42637\n",
      "msa\t\n",
      "\n",
      "msa\t\n",
      "\n",
      "msa\t\n",
      "\n",
      "msa\t\n",
      "\n",
      "msa\t\n",
      "\n",
      "msa\t\n",
      "\n",
      "msa\t\n",
      "\n",
      "msa\t\n",
      "\n",
      "msa\t                                         \n",
      "\n",
      "msa\t                                         \n",
      "\n",
      "msa\t\n",
      "\n",
      "msa\t\n",
      "\n",
      "msa\t\n",
      "\n",
      "msa\t\n",
      "\n",
      "msa\t\n",
      "\n",
      "msa\t\n",
      "\n",
      "msa\t\n",
      "\n",
      "msa\t\n",
      "\n",
      "msa\t\n",
      "\n",
      "msa\t\n",
      "\n",
      "64711\n",
      "/home/disooqi/qcri/dialects/data/AOC.tar\n",
      "/home/disooqi/qcri/dialects/data/MultiDial.zip\n",
      "/home/disooqi/qcri/dialects/data/annotated_data.tar.gz\n",
      "/home/disooqi/qcri/dialects/data/AOC-dialectal-annotations.zip\n",
      "/home/disooqi/qcri/dialects/data/AOC-dialectal-annotations/dialect_alghad-segs_ids.txt\n",
      "/home/disooqi/qcri/dialects/data/AOC-dialectal-annotations/MSA_youm7-segs.txt.norm\n",
      "/home/disooqi/qcri/dialects/data/AOC-dialectal-annotations/dialect_youm7-segs_ids.txt\n",
      "/home/disooqi/qcri/dialects/data/AOC-dialectal-annotations/AOC_sample_articles.xml\n",
      "/home/disooqi/qcri/dialects/data/AOC-dialectal-annotations/MSA_youm7-segs_ids.txt\n",
      "/home/disooqi/qcri/dialects/data/AOC-dialectal-annotations/dialect_alriyadh-segs_ids.txt\n",
      "/home/disooqi/qcri/dialects/data/AOC-dialectal-annotations/MSA_alghad-segs_ids.txt\n",
      "/home/disooqi/qcri/dialects/data/AOC-dialectal-annotations/dialect_youm7-segs.txt.norm\n",
      "/home/disooqi/qcri/dialects/data/AOC-dialectal-annotations/dialect_alriyadh-segs.txt.norm\n",
      "/home/disooqi/qcri/dialects/data/AOC-dialectal-annotations/MSA_alghad-segs.txt.norm\n",
      "/home/disooqi/qcri/dialects/data/AOC-dialectal-annotations/MSA_alriyadh-segs_ids.txt\n",
      "/home/disooqi/qcri/dialects/data/AOC-dialectal-annotations/README.txt\n",
      "/home/disooqi/qcri/dialects/data/AOC-dialectal-annotations/MSA_alriyadh-segs.txt.norm\n",
      "/home/disooqi/qcri/dialects/data/AOC-dialectal-annotations/dialect_alghad-segs.txt.norm\n",
      "/home/disooqi/qcri/dialects/data/AOC-dialectal-annotations/AOC_sample_comments.xml\n",
      "/home/disooqi/qcri/dialects/data/annotated_data/twitter-iraqi\n",
      "/home/disooqi/qcri/dialects/data/annotated_data/egyptian\n",
      "/home/disooqi/qcri/dialects/data/annotated_data/msa\n",
      "/home/disooqi/qcri/dialects/data/annotated_data/maghrebi\n",
      "/home/disooqi/qcri/dialects/data/annotated_data/twitter-levantine\n",
      "/home/disooqi/qcri/dialects/data/annotated_data/gulf\n",
      "/home/disooqi/qcri/dialects/data/annotated_data/README~\n",
      "/home/disooqi/qcri/dialects/data/annotated_data/twitter-maghrebi\n",
      "/home/disooqi/qcri/dialects/data/annotated_data/iraqi\n",
      "/home/disooqi/qcri/dialects/data/annotated_data/README\n",
      "/home/disooqi/qcri/dialects/data/annotated_data/levantine\n",
      "/home/disooqi/qcri/dialects/data/annotated_data/twitter-egyptian\n",
      "/home/disooqi/qcri/dialects/data/annotated_data/twitter-gulf\n",
      "/home/disooqi/qcri/dialects/data/AOC/AOC_sample_articles.xml\n",
      "/home/disooqi/qcri/dialects/data/AOC/.DS_Store\n",
      "/home/disooqi/qcri/dialects/data/AOC/AOC_readme.txt\n",
      "/home/disooqi/qcri/dialects/data/AOC/AOC_v1.0.zip\n",
      "/home/disooqi/qcri/dialects/data/AOC/AOC_v1.1.zip\n",
      "/home/disooqi/qcri/dialects/data/AOC/AOC_sample_comments.xml\n",
      "/home/disooqi/qcri/dialects/data/AOC/AOC_v1.1/AOC_alriyadh_articles.xml\n",
      "/home/disooqi/qcri/dialects/data/AOC/AOC_v1.1/AOC_youm7_comments.xml\n",
      "/home/disooqi/qcri/dialects/data/AOC/AOC_v1.1/AOC_youm7_articles.xml\n",
      "/home/disooqi/qcri/dialects/data/AOC/AOC_v1.1/AOC_alghad_comments.xml\n",
      "/home/disooqi/qcri/dialects/data/AOC/AOC_v1.1/AOC_alriyadh-sample_comments.xml\n",
      "/home/disooqi/qcri/dialects/data/AOC/AOC_v1.1/AOC_youm7-sample_comments.xml\n",
      "/home/disooqi/qcri/dialects/data/AOC/AOC_v1.1/AOC_alriyadh_comments.xml\n",
      "/home/disooqi/qcri/dialects/data/AOC/AOC_v1.1/AOC_sample_articles.xml\n",
      "/home/disooqi/qcri/dialects/data/AOC/AOC_v1.1/AOC_alghad-sample_articles.xml\n",
      "/home/disooqi/qcri/dialects/data/AOC/AOC_v1.1/AOC_readme.txt\n",
      "/home/disooqi/qcri/dialects/data/AOC/AOC_v1.1/AOC_alghad-sample_comments.xml\n",
      "/home/disooqi/qcri/dialects/data/AOC/AOC_v1.1/AOC_alriyadh-sample_articles.xml\n",
      "/home/disooqi/qcri/dialects/data/AOC/AOC_v1.1/AOC_sample_comments.xml\n",
      "/home/disooqi/qcri/dialects/data/AOC/AOC_v1.1/AOC_youm7-sample_articles.xml\n",
      "/home/disooqi/qcri/dialects/data/AOC/AOC_v1.1/AOC_alghad_articles.xml\n",
      "/home/disooqi/qcri/dialects/data/AOC/dialect-classification/dialect-classification_batch2_2011-01-18.zip\n",
      "/home/disooqi/qcri/dialects/data/AOC/dialect-classification/dialect-classification_batch2_2010-12-31.zip\n",
      "/home/disooqi/qcri/dialects/data/AOC/dialect-classification/dialect-classification_batch2_2011-01-30.zip\n",
      "/home/disooqi/qcri/dialects/data/AOC/dialect-classification/dialect-classification_batch2_2011-01-23.zip\n",
      "/home/disooqi/qcri/dialects/data/AOC/dialect-classification/dialect-classification_batch2_2011-01-08.zip\n",
      "/home/disooqi/qcri/dialects/data/AOC/dialect-classification/dialect-classification_batch2_2011-01-12.zip\n",
      "/home/disooqi/qcri/dialects/data/AOC/dialect-classification/dialect-classification_batch2_2010-12-20.zip\n",
      "/home/disooqi/qcri/dialects/data/AOC/dialect-classification/dialect-classification_batch2_2010-12-26.zip\n",
      "/home/disooqi/qcri/dialects/data/MultiDial/__MACOSX/MultiDial/._TN\n",
      "/home/disooqi/qcri/dialects/data/MultiDial/__MACOSX/MultiDial/._EG\n",
      "/home/disooqi/qcri/dialects/data/MultiDial/__MACOSX/MultiDial/._JO\n",
      "/home/disooqi/qcri/dialects/data/MultiDial/MultiDial/JO\n",
      "/home/disooqi/qcri/dialects/data/MultiDial/MultiDial/EG\n",
      "/home/disooqi/qcri/dialects/data/MultiDial/MultiDial/EN\n",
      "/home/disooqi/qcri/dialects/data/MultiDial/MultiDial/MSA\n",
      "/home/disooqi/qcri/dialects/data/MultiDial/MultiDial/PA\n",
      "/home/disooqi/qcri/dialects/data/MultiDial/MultiDial/SY\n",
      "/home/disooqi/qcri/dialects/data/MultiDial/MultiDial/TN\n"
     ]
    }
   ],
   "source": [
    "annotated_data = {'LAV':list(),'MSA':list(),'EGY':list(),'GLF':list(),'NOR':list()}\n",
    "AOC = {'LAV':list(),'MSA':list(),'EGY':list(),'GLF':list(),'NOR':list()}\n",
    "MultiDial = {'LAV':list(),'MSA':list(),'EGY':list(),'GLF':list(),'NOR':list()}\n",
    "\n",
    "with codecs.open(r'/home/disooqi/qcri/dialects/data/annotated_data/egyptian', encoding='utf8') as dialfile:\n",
    "    for line in dialfile:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) !=2:\n",
    "            print line\n",
    "            continue\n",
    "        annotated_data['EGY'].append(parts[1])\n",
    "print len(annotated_data['EGY'])\n",
    "with codecs.open(r'/home/disooqi/qcri/dialects/data/annotated_data/twitter-egyptian', encoding='utf8') as dialfile:\n",
    "    for line in dialfile:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) !=2:\n",
    "            print line\n",
    "            continue\n",
    "        annotated_data['EGY'].append(parts[1])\n",
    "print len(annotated_data['EGY'])\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "with codecs.open(r'/home/disooqi/qcri/dialects/data/annotated_data/maghrebi', encoding='utf8') as dialfile:\n",
    "    for line in dialfile:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) !=2:\n",
    "            print line\n",
    "            continue\n",
    "        annotated_data['NOR'].append(parts[1])\n",
    "print len(annotated_data['NOR'])\n",
    "with codecs.open(r'/home/disooqi/qcri/dialects/data/annotated_data/twitter-maghrebi', encoding='utf8') as dialfile:\n",
    "    for line in dialfile:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) !=2:\n",
    "            print line\n",
    "            continue\n",
    "        annotated_data['NOR'].append(parts[1])\n",
    "print len(annotated_data['NOR'])\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "with codecs.open(r'/home/disooqi/qcri/dialects/data/annotated_data/levantine', encoding='utf8') as dialfile:\n",
    "    for line in dialfile:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) !=2:\n",
    "            print line\n",
    "            continue\n",
    "        annotated_data['LAV'].append(parts[1])\n",
    "print len(annotated_data['LAV'])\n",
    "with codecs.open(r'/home/disooqi/qcri/dialects/data/annotated_data/twitter-levantine', encoding='utf8') as dialfile:\n",
    "    for line in dialfile:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) !=2:\n",
    "            print line\n",
    "            continue\n",
    "        annotated_data['LAV'].append(parts[1])\n",
    "print len(annotated_data['LAV'])\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "with codecs.open(r'/home/disooqi/qcri/dialects/data/annotated_data/gulf', encoding='utf8') as dialfile:\n",
    "    for line in dialfile:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) !=2:\n",
    "            print line\n",
    "            continue\n",
    "        annotated_data['GLF'].append(parts[1])\n",
    "print len(annotated_data['GLF'])\n",
    "with codecs.open(r'/home/disooqi/qcri/dialects/data/annotated_data/twitter-gulf', encoding='utf8') as dialfile:\n",
    "    for line in dialfile:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) !=2:\n",
    "            print line\n",
    "            continue\n",
    "        annotated_data['GLF'].append(parts[1])\n",
    "print len(annotated_data['GLF'])\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "with codecs.open(r'/home/disooqi/qcri/dialects/data/annotated_data/msa', encoding='utf8') as dialfile:\n",
    "    for line in dialfile:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) !=2:\n",
    "            print line\n",
    "            continue\n",
    "        annotated_data['MSA'].append(parts[1])\n",
    "print len(annotated_data['MSA'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print annotated_data['EGY']\n",
    "for the_dir, subdir, files in os.walk(r'/home/disooqi/qcri/dialects/data/'):\n",
    "    for f in files:\n",
    "#         if f.endswith('.txt'):\n",
    "        file_path = os.path.join(the_dir, f)\n",
    "        print file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6405"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min([len(l) for l in annotated_data.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAV training dataset:  6405 , cross-validation set: 00, test: 00\n",
      "MSA training dataset:  6405 , cross-validation set: 00, test: 00\n",
      "EGY training dataset:  6405 , cross-validation set: 00, test: 00\n",
      "GLF training dataset:  6405 , cross-validation set: 00, test: 00\n",
      "NOR training dataset:  6405 , cross-validation set: 00, test: 00\n",
      "----------------------------------------------------------------------\n",
      "Total  ...  Training:  32025 , cross-validation data 0 , test:  0\n"
     ]
    }
   ],
   "source": [
    "def divide_dataset(dataset ,CV=True, train_perc=80 , CV_perc=0, test_perc=20):\n",
    "    if train_perc + CV_perc + test_perc != 100:\n",
    "        print 'the sum of percs is not 100'\n",
    "        return\n",
    "    samples_train = dict()\n",
    "    samples_cv = dict()\n",
    "    samples_test = dict()\n",
    "    \n",
    "    train_len = min([len(l) for l in annotated_data.values()])\n",
    "        \n",
    "    \n",
    "    for dialect, sentences in dataset.items():\n",
    "        samples = permutation(sentences)\n",
    "        #train_len = int(np.ceil(len(samples)*(train_perc/100.0)))\n",
    "        samples_train[dialect] = sentences[:train_len]\n",
    "#         cv_len = 0\n",
    "#         if CV:\n",
    "#             cvp = CV_perc/(100.0-60)\n",
    "#             cv_len = int(np.ceil((len(samples)-train_len) * cvp))\n",
    "#             samples_cv[dialect] = sentences[train_len:train_len+cv_len]\n",
    "#             samples_test[dialect] = sentences[train_len+cv_len:]\n",
    "#         else:\n",
    "#             samples_cv[dialect] = list()\n",
    "#             samples_test[dialect] = sentences[train_len:]\n",
    "    else:\n",
    "        return samples_train, samples_cv, samples_test\n",
    "            \n",
    "\n",
    "train_set, cv_set, test_set = divide_dataset(annotated_data, CV=False, train_perc=100 ,CV_perc=0, test_perc=0)\n",
    "\n",
    "target_names = ['LAV', 'MSA', 'EGY', 'GLF', 'NOR']\n",
    "\n",
    "t,c,ts = 0,0,0\n",
    "for dial in ['LAV', 'MSA', 'EGY', 'GLF', 'NOR']:\n",
    "    t += len(train_set[dial])\n",
    "#     c += len(cv_set[dial])\n",
    "#     ts+= len(test_set[dial])\n",
    "    print dial, 'training dataset: ', len(train_set[dial]), ', cross-validation set: 00, test: 00'\n",
    "    \n",
    "else:\n",
    "    print 70*'-'\n",
    "    print 'Total  ...  Training: ', t, ', cross-validation data', c, ', test: ', ts\n",
    "\n",
    "dataset_train = train_set['LAV']+train_set['MSA']+train_set['EGY']+train_set['GLF']+train_set['NOR']\n",
    "# dataset_cv = cv_set['LAV']+cv_set['MSA']+cv_set['EGY']+cv_set['GLF']+cv_set['NOR']\n",
    "# dataset_test = test_set['LAV']+test_set['MSA']+test_set['EGY']+test_set['GLF']+test_set['NOR']\n",
    "\n",
    "\n",
    "label_train = ['LAV' for x in train_set['LAV']] + ['MSA' for x in train_set['MSA']] +\\\n",
    "['EGY' for x in train_set['EGY']] + ['GLF' for x in train_set['GLF']]+['NOR' for x in train_set['NOR']]\n",
    "\n",
    "# label_cv = ['LAV' for x in cv_set['LAV']] + ['MSA' for x in cv_set['MSA']] +\\\n",
    "# ['EGY' for x in cv_set['EGY']] + ['GLF' for x in cv_set['GLF']]+['NOR' for x in cv_set['NOR']]\n",
    "\n",
    "# label_test = ['LAV' for x in test_set['LAV']] + ['MSA' for x in test_set['MSA']] +\\\n",
    "# ['EGY' for x in test_set['EGY']] + ['GLF' for x in test_set['GLF']]+['NOR' for x in test_set['NOR']]\n",
    "\n",
    "train_set, cv_set, test_set = 0,0,0\n",
    "#print len(label_train),len(label_cv),len(label_test)\n",
    "\n",
    "train_zipped = zip(dataset_train, label_train)\n",
    "random.shuffle(train_zipped)\n",
    "dataset_train, label_train = zip(*train_zipped)\n",
    "\n",
    "# if dataset_cv:\n",
    "#     cv_zipped = zip(dataset_cv, label_cv)\n",
    "#     random.shuffle(cv_zipped)\n",
    "#     dataset_cv, label_cv = zip(*cv_zipped)\n",
    "\n",
    "# if dataset_test:\n",
    "#     dataset_test.extend(removed_sentences)\n",
    "#     label_test.extend(removed_sentences_labels)\n",
    "#     test_zipped = zip(dataset_test, label_test)\n",
    "#     random.shuffle(test_zipped)\n",
    "#     dataset_test, label_test = zip(*test_zipped)\n",
    "\n",
    "# print len(dataset_test), len(removed_sentences), len(label_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAV training dataset:  1758 , cross-validation set:  0 , test: 0\n",
      "MSA training dataset:  999 , cross-validation set:  0 , test: 0\n",
      "EGY training dataset:  1578 , cross-validation set:  0 , test: 0\n",
      "GLF training dataset:  1672 , cross-validation set:  0 , test: 0\n",
      "NOR training dataset:  1612 , cross-validation set:  0 , test: 0\n",
      "----------------------------------------------------------------------\n",
      "Total  ...  Training:  7619 , cross-validation data 0 , test:  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(47263, 47263)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = list()\n",
    "removed_sentences = list()\n",
    "removed_sentences_labels = list()\n",
    "labels = list()\n",
    "\n",
    "labels_dist = set()\n",
    "\n",
    "dataset = dict()\n",
    "#We will release training and testing data for the following Arabic dialects: \n",
    "# Egyptian, Gulf, Levantine, and North-African, and Modern Standard Arabic (MSA)\n",
    "\n",
    "with codecs.open('/home/disooqi/qcri/dialects/task/DSL-training/task2-train.txt') as training:\n",
    "    LAV = list()\n",
    "    MSA = list()\n",
    "    EGY = list()\n",
    "    GLF = list()\n",
    "    NOR = list()\n",
    "    for i, line in enumerate(training):\n",
    "        sentence_label = line.strip().split('\\t')\n",
    "        utf8_sentence = sentence_label[0]\n",
    "        \n",
    "        # labels.append(sentence_label[2])\n",
    "        \n",
    "        if len(utf8_sentence.strip().split()) <= 0:\n",
    "            removed_sentences.append(utf8_sentence)\n",
    "            removed_sentences_labels.append(sentence_label[2])\n",
    "#             print i, sentence_label[0]\n",
    "            continue\n",
    "        \n",
    "        sentences.append(utf8_sentence)\n",
    "        if sentence_label[2] == 'LAV':\n",
    "            LAV.append(utf8_sentence)\n",
    "        elif sentence_label[2] == 'MSA':\n",
    "            MSA.append(utf8_sentence)\n",
    "        elif sentence_label[2] == 'EGY':\n",
    "            EGY.append(utf8_sentence)\n",
    "        elif sentence_label[2] == 'GLF':\n",
    "            GLF.append(utf8_sentence)\n",
    "        elif sentence_label[2] == 'NOR':\n",
    "            NOR.append(utf8_sentence)\n",
    "        else:\n",
    "            print(utf8_sentence)\n",
    "    else:\n",
    "#         print 'sentence count:', len(sentences)\n",
    "#         print set(labels)\n",
    "        dataset['LAV'] = LAV\n",
    "        dataset['MSA'] = MSA\n",
    "        dataset['EGY'] = EGY\n",
    "        dataset['GLF'] = GLF\n",
    "        dataset['NOR'] = NOR\n",
    "        LAV = list()\n",
    "        MSA = list()\n",
    "        EGY = list()\n",
    "        GLF = list()\n",
    "        NOR = list()\n",
    "\n",
    "\n",
    "def divide_dataset__2(dataset ,CV=True, train_perc=80 , CV_perc=0, test_perc=20):\n",
    "    if train_perc + CV_perc + test_perc != 100:\n",
    "        print 'the sum of percs is not 100'\n",
    "        return\n",
    "    samples_train = dict()\n",
    "    samples_cv = dict()\n",
    "    samples_test = dict()\n",
    "    \n",
    "    for dialect, sentences in dataset.items():\n",
    "        samples = permutation(sentences)\n",
    "        train_len = int(np.ceil(len(samples)*(train_perc/100.0)))\n",
    "        samples_train[dialect] = sentences[:train_len]\n",
    "        cv_len = 0\n",
    "        if CV:\n",
    "            cvp = CV_perc/(100.0-60)\n",
    "            cv_len = int(np.ceil((len(samples)-train_len) * cvp))\n",
    "            samples_cv[dialect] = sentences[train_len:train_len+cv_len]\n",
    "            samples_test[dialect] = sentences[train_len+cv_len:]\n",
    "        else:\n",
    "            samples_cv[dialect] = list()\n",
    "            samples_test[dialect] = sentences[train_len:]\n",
    "    else:\n",
    "        return samples_train, samples_cv, samples_test\n",
    "            \n",
    "\n",
    "train_set, cv_set, test_set = divide_dataset__2(dataset, CV=False, train_perc=100 ,CV_perc=0, test_perc=0)\n",
    "\n",
    "\n",
    "t,c,ts = 0,0,0\n",
    "for dial in target_names:\n",
    "    t += len(train_set[dial])\n",
    "    c += len(cv_set[dial])\n",
    "    ts+= len(test_set[dial])\n",
    "    print dial, 'training dataset: ', len(train_set[dial]), ', cross-validation set: ', \\\n",
    "    len(cv_set[dial]),', test:', len(test_set[dial])\n",
    "    \n",
    "else:\n",
    "    print 70*'-'\n",
    "    print 'Total  ...  Training: ', t, ', cross-validation data', c, ', test: ', ts\n",
    "dataset_train = list(dataset_train)\n",
    "dataset_train.extend(train_set['LAV']+train_set['MSA']+train_set['EGY']+train_set['GLF']+train_set['NOR'])\n",
    "\n",
    "label_train = list(label_train)\n",
    "label_train.extend(['LAV' for x in train_set['LAV']] + ['MSA' for x in train_set['MSA']] +\n",
    "['EGY' for x in train_set['EGY']] + ['GLF' for x in train_set['GLF']]+['NOR' for x in train_set['NOR']])\n",
    "\n",
    "\n",
    "train_zipped = zip(dataset_train, label_train)\n",
    "random.shuffle(train_zipped)\n",
    "dataset_train, label_train = zip(*train_zipped)\n",
    "\n",
    "len(dataset_train), len(label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sentence count: 1540\n"
     ]
    }
   ],
   "source": [
    "test_set_path = r'/home/disooqi/qcri/dialects/task/DSL2016-test/C.txt'\n",
    "test_sentences = list()\n",
    "test_sentences2 = list()\n",
    "with codecs.open(test_set_path) as testf:\n",
    "    for i, line in enumerate(testf):\n",
    "#         if len(line.strip().split()) <= 1:\n",
    "#             print i, from_buck_to_utf8(line.strip())\n",
    "#             continue\n",
    "        test_sentences.append(line.strip())\n",
    "        test_sentences2.append(from_buck_to_utf8(line.strip()))\n",
    "    else:\n",
    "        print 'Test sentence count:', len(test_sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Acc:  88.16 %\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer= 'char',lowercase=False, max_df=0.95,ngram_range=(2,5), smooth_idf=False,\n",
    "                             sublinear_tf=True)\n",
    "\n",
    "# vectorizer.fit(sentences)\n",
    "X_train = vectorizer.fit_transform(dataset_train)\n",
    "X_test = vectorizer.transform(test_sentences2)\n",
    "\n",
    "sgd_clf_02 = SGDClassifier()\n",
    "sgd_clf_02.fit(X_train, label_train)\n",
    "\n",
    "pred_train = sgd_clf_02.predict(X_train)\n",
    "pred_test = sgd_clf_02.predict(X_test)\n",
    "\n",
    "\n",
    "with codecs.open('QCRI-open-C-run2.txt', mode='w', encoding='utf8') as outfile:\n",
    "    if len(test_sentences) != len(pred_test):\n",
    "        print 'something wrong'\n",
    "    else:\n",
    "        for ts, tl in zip(test_sentences, pred_test):\n",
    "            outfile.write(ts)\n",
    "            outfile.write('\\t')\n",
    "            outfile.write(tl)\n",
    "            outfile.write('\\n')\n",
    "            \n",
    "\n",
    "print 'Training Acc: ',np.around(np.mean(pred_train == label_train)*100,2), '%'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

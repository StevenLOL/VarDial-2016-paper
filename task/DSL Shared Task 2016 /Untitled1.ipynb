{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggestions:\n",
    "1. http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "* https://github.com/timshenkao/StringKernelSVM\n",
    "\n",
    "### Use the following to show off the results\n",
    "http://melissagymrek.com/python/2014/01/12/ipython-tables.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import codecs\n",
    "from collections import Counter\n",
    "import random \n",
    "import numpy as np\n",
    "from numpy.random import permutation, shuffle, rand\n",
    "from numpy.linalg import svd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def from_buck_to_utf8(text):\n",
    "    b2a = {'A': u'\\u0627',  '<': u'\\u0625',  '|': u'\\u0622',  '>': u'\\u0623',  \"'\": u'\\u0621',  'b': u'\\u0628',  \n",
    "           't': u'\\u062a',  'v': u'\\u062b',  'j': u'\\u062c',  'H': u'\\u062d',  'x': u'\\u062e',  'd': u'\\u062f',  \n",
    "           '*': u'\\u0630',  'r': u'\\u0631',  'z': u'\\u0632',  's': u'\\u0633',  '$': u'\\u0634',  'S': u'\\u0635',  \n",
    "           'D': u'\\u0636',  'T': u'\\u0637',  'Z': u'\\u0638',  'E': u'\\u0639',  'g': u'\\u063a',  'f': u'\\u0641',  \n",
    "           'q': u'\\u0642',  'k': u'\\u0643',  'l': u'\\u0644',  'm': u'\\u0645',  'n': u'\\u0646',  'h': u'\\u0647',  \n",
    "           'w': u'\\u0648',  'y': u'\\u064a',  'Y': u'\\u0649',  'p': u'\\u0629',  '&': u'\\u0624',  '}': u'\\u0626',  \n",
    "           'a': u'\\u064e',  'F': u'\\u064b',  'u': u'\\u064f',  'N': u'\\u064c',  'i': u'\\u0650',  'K': u'\\u064d',  \n",
    "           'o': u'\\u0652',  '~': u'\\u0651'}\n",
    "    text = text.strip().split()\n",
    "    tmp_sentence = list()\n",
    "    for word in text:\n",
    "        tmp_word = list()\n",
    "        for c in word:\n",
    "            tmp_word.append(b2a.get(c,c))\n",
    "        else:\n",
    "            tmp_sentence.append(''.join(tmp_word))\n",
    "    else:\n",
    "        return ' '.join(tmp_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOR', 'LAV', 'GLF', 'EGY', 'MSA']\n",
      "LAV training dataset:  1407 , cross-validation set:  0 , test: 351\n",
      "MSA training dataset:  800 , cross-validation set:  0 , test: 199\n",
      "EGY training dataset:  1263 , cross-validation set:  0 , test: 315\n",
      "GLF training dataset:  1338 , cross-validation set:  0 , test: 334\n",
      "NOR training dataset:  1290 , cross-validation set:  0 , test: 322\n",
      "----------------------------------------------------------------------\n",
      "Total  ...  Training:  6098 , cross-validation data 0 , test:  1521\n",
      "1521 0 1521\n"
     ]
    }
   ],
   "source": [
    "sentences = list()\n",
    "removed_sentences = list()\n",
    "removed_sentences_labels = list()\n",
    "labels = list()\n",
    "\n",
    "labels_dist = set()\n",
    "\n",
    "dataset = dict()\n",
    "#We will release training and testing data for the following Arabic dialects: \n",
    "# Egyptian, Gulf, Levantine, and North-African, and Modern Standard Arabic (MSA)\n",
    "\n",
    "with codecs.open('/home/disooqi/qcri/dialects/task/DSL-training/task2-train.txt') as training:\n",
    "    LAV = list()\n",
    "    MSA = list()\n",
    "    EGY = list()\n",
    "    GLF = list()\n",
    "    NOR = list()\n",
    "    for i, line in enumerate(training):\n",
    "        sentence_label = line.strip().split('\\t')\n",
    "        utf8_sentence = sentence_label[0]\n",
    "        \n",
    "        # labels.append(sentence_label[2])\n",
    "        \n",
    "        if len(utf8_sentence.strip().split()) <= 0:\n",
    "            removed_sentences.append(utf8_sentence)\n",
    "            removed_sentences_labels.append(sentence_label[2])\n",
    "#             print i, sentence_label[0]\n",
    "            continue\n",
    "        \n",
    "        sentences.append(utf8_sentence)\n",
    "        if sentence_label[2] == 'LAV':\n",
    "            LAV.append(utf8_sentence)\n",
    "        elif sentence_label[2] == 'MSA':\n",
    "            MSA.append(utf8_sentence)\n",
    "        elif sentence_label[2] == 'EGY':\n",
    "            EGY.append(utf8_sentence)\n",
    "        elif sentence_label[2] == 'GLF':\n",
    "            GLF.append(utf8_sentence)\n",
    "        elif sentence_label[2] == 'NOR':\n",
    "            NOR.append(utf8_sentence)\n",
    "        else:\n",
    "            print(utf8_sentence)\n",
    "    else:\n",
    "#         print 'sentence count:', len(sentences)\n",
    "#         print set(labels)\n",
    "        dataset['LAV'] = LAV\n",
    "        dataset['MSA'] = MSA\n",
    "        dataset['EGY'] = EGY\n",
    "        dataset['GLF'] = GLF\n",
    "        dataset['NOR'] = NOR\n",
    "        LAV = list()\n",
    "        MSA = list()\n",
    "        EGY = list()\n",
    "        GLF = list()\n",
    "        NOR = list()\n",
    "\n",
    "target_names = dataset.keys()\n",
    "print target_names\n",
    "\n",
    "def divide_dataset(dataset ,CV=True, train_perc=80 , CV_perc=0, test_perc=20):\n",
    "    if train_perc + CV_perc + test_perc != 100:\n",
    "        print 'the sum of percs is not 100'\n",
    "        return\n",
    "    samples_train = dict()\n",
    "    samples_cv = dict()\n",
    "    samples_test = dict()\n",
    "    \n",
    "    for dialect, sentences in dataset.items():\n",
    "        samples = permutation(sentences)\n",
    "        train_len = int(np.ceil(len(samples)*(train_perc/100.0)))\n",
    "        samples_train[dialect] = sentences[:train_len]\n",
    "        cv_len = 0\n",
    "        if CV:\n",
    "            cvp = CV_perc/(100.0-60)\n",
    "            cv_len = int(np.ceil((len(samples)-train_len) * cvp))\n",
    "            samples_cv[dialect] = sentences[train_len:train_len+cv_len]\n",
    "            samples_test[dialect] = sentences[train_len+cv_len:]\n",
    "        else:\n",
    "            samples_cv[dialect] = list()\n",
    "            samples_test[dialect] = sentences[train_len:]\n",
    "    else:\n",
    "        return samples_train, samples_cv, samples_test\n",
    "            \n",
    "\n",
    "train_set, cv_set, test_set = divide_dataset(dataset, CV=False, train_perc=80 ,CV_perc=0, test_perc=20)\n",
    "\n",
    "target_names = ['LAV', 'MSA', 'EGY', 'GLF', 'NOR']\n",
    "\n",
    "t,c,ts = 0,0,0\n",
    "for dial in ['LAV', 'MSA', 'EGY', 'GLF', 'NOR']:\n",
    "    t += len(train_set[dial])\n",
    "    c += len(cv_set[dial])\n",
    "    ts+= len(test_set[dial])\n",
    "    print dial, 'training dataset: ', len(train_set[dial]), ', cross-validation set: ', \\\n",
    "    len(cv_set[dial]),', test:', len(test_set[dial])\n",
    "    \n",
    "else:\n",
    "    print 70*'-'\n",
    "    print 'Total  ...  Training: ', t, ', cross-validation data', c, ', test: ', ts\n",
    "\n",
    "dataset_train = train_set['LAV']+train_set['MSA']+train_set['EGY']+train_set['GLF']+train_set['NOR']\n",
    "dataset_cv = cv_set['LAV']+cv_set['MSA']+cv_set['EGY']+cv_set['GLF']+cv_set['NOR']\n",
    "dataset_test = test_set['LAV']+test_set['MSA']+test_set['EGY']+test_set['GLF']+test_set['NOR']\n",
    "\n",
    "\n",
    "label_train = ['LAV' for x in train_set['LAV']] + ['MSA' for x in train_set['MSA']] +\\\n",
    "['EGY' for x in train_set['EGY']] + ['GLF' for x in train_set['GLF']]+['NOR' for x in train_set['NOR']]\n",
    "\n",
    "label_cv = ['LAV' for x in cv_set['LAV']] + ['MSA' for x in cv_set['MSA']] +\\\n",
    "['EGY' for x in cv_set['EGY']] + ['GLF' for x in cv_set['GLF']]+['NOR' for x in cv_set['NOR']]\n",
    "\n",
    "label_test = ['LAV' for x in test_set['LAV']] + ['MSA' for x in test_set['MSA']] +\\\n",
    "['EGY' for x in test_set['EGY']] + ['GLF' for x in test_set['GLF']]+['NOR' for x in test_set['NOR']]\n",
    "\n",
    "train_set, cv_set, test_set = 0,0,0\n",
    "#print len(label_train),len(label_cv),len(label_test)\n",
    "\n",
    "train_zipped = zip(dataset_train, label_train)\n",
    "random.shuffle(train_zipped)\n",
    "dataset_train, label_train = zip(*train_zipped)\n",
    "\n",
    "if dataset_cv:\n",
    "    cv_zipped = zip(dataset_cv, label_cv)\n",
    "    random.shuffle(cv_zipped)\n",
    "    dataset_cv, label_cv = zip(*cv_zipped)\n",
    "\n",
    "if dataset_test:\n",
    "    dataset_test.extend(removed_sentences)\n",
    "    label_test.extend(removed_sentences_labels)\n",
    "    test_zipped = zip(dataset_test, label_test)\n",
    "    random.shuffle(test_zipped)\n",
    "    dataset_test, label_test = zip(*test_zipped)\n",
    "\n",
    "print len(dataset_test), len(removed_sentences), len(label_test)\n",
    "\n",
    "with codecs.open('LIBSVM_train', mode='w', encoding='utf8') as of:\n",
    "    for l, s in zip(label_train, dataset_train):\n",
    "        of.write(str(target_names.index(l)))\n",
    "        of.write('\\t')\n",
    "        of.write(s)\n",
    "        of.write('\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with codecs.open('LIBSVM_test', mode='w', encoding='utf8') as of:\n",
    "    for l, s in zip(label_test, dataset_test):\n",
    "        of.write(str(target_names.index(l)))\n",
    "        of.write('\\t')\n",
    "        of.write(s)\n",
    "        of.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7619\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "print len(sentences)\n",
    "print len(removed_sentences), len(removed_sentences_labels)\n",
    "\n",
    "for t in zip(removed_sentences, removed_sentences_labels):\n",
    "    print t[0], t[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def randInitializeWeights(L_in_size, L_out_size):\n",
    "    epsilon_init = np.sqrt(6)/np.sqrt(L_in_size+L_out_size)\n",
    "    epsilon_init = 0.12\n",
    "    return  rand(L_out_size, L_in_size+1) * 2*epsilon_init - epsilon_init\n",
    "\n",
    "def sigmoid(z): \n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def sigmoidGradient(z):\n",
    "    return sigmoid(z) * (1-sigmoid(z))\n",
    "\n",
    "def nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, features, incidence_y, reg_parameter):\n",
    "    if nn_params.ndim != 1:\n",
    "        return\n",
    "    theta1_size = (input_layer_size+1) * hidden_layer_size\n",
    "    Theta1 = nn_params[:theta1_size].reshape((hidden_layer_size,input_layer_size+1), order='F') # (25, 401)\n",
    "    Theta2 = nn_params[theta1_size:].reshape((num_labels, hidden_layer_size+1), order='F') # (10, 26)\n",
    "    \n",
    "    m, _ = features.shape\n",
    "    a_1 = np.c_[np.ones((m)), features]\n",
    "    \n",
    "    z_2 = Theta1.dot(a_1.T) # (25, 401) * (401, 5000)\n",
    "    a_tmp = sigmoid(z_2)    # (25, 5000)\n",
    "    \n",
    "    a_2 = np.vstack((np.ones((m)), a_tmp))\n",
    "    z_3 = Theta2.dot(a_2)\n",
    "    a_3 = sigmoid(z_3)\n",
    "    \n",
    "    reg_term = _lambda *(np.sum(Theta1[:,1:]**2) + np.sum(Theta2[:,1:]**2))/(2*m)\n",
    "    error =  np.sum(-incidence_y*np.log(a_3.T) - (1-incidence_y)*np.log(1 - a_3.T))/m\n",
    "    print 'Train error: ', error\n",
    "    return error +reg_term\n",
    "\n",
    "\n",
    "def nn_gradient(nn_params, input_layer_size, hidden_layer_size, num_labels,features, incidence_y, _lambda):\n",
    "    m, _ = features.shape\n",
    "    ones = np.ones((m))\n",
    "    # print ones.shape, type(features), features.shape\n",
    "    X = np.c_[np.ones((m)), features]\n",
    "    \n",
    "    if nn_params.ndim != 1:\n",
    "        return\n",
    "    theta1_size = (input_layer_size+1) * hidden_layer_size\n",
    "    Theta1 = nn_params[:theta1_size].reshape((hidden_layer_size,input_layer_size+1), order='F') # (25, 401)\n",
    "    Theta2 = nn_params[theta1_size:].reshape((num_labels, hidden_layer_size+1), order='F') # (10, 26)\n",
    "\n",
    "    Delta2 = np.zeros_like(Theta2)\n",
    "    Delta1 = np.zeros_like(Theta1)\n",
    "\n",
    "    for i in np.arange(m):        \n",
    "        # forward pass\n",
    "        x = X[i,:]\n",
    "    \n",
    "        z2 = Theta1.dot(x[:,np.newaxis])\n",
    "        a2 = np.r_[[[1]], sigmoid(z2)]\n",
    "    \n",
    "        z3 = Theta2.dot(a2)\n",
    "        hx = sigmoid(z3).ravel()\n",
    "    \n",
    "        # computing the \"error terms\" that measure how much the nodes were responsible for any errors \n",
    "        # in our output\n",
    "        delta3 = hx - incidence_y[i,:]\n",
    "        delta2 = Theta2.T.dot(delta3)[1:] * sigmoidGradient(z2).ravel()\n",
    "    \n",
    "        Delta2 = Delta2 + delta3[:,np.newaxis].dot(a2.T)\n",
    "        Delta1 = Delta1 + delta2[:,np.newaxis].dot(x[:,np.newaxis].T)\n",
    "    else:\n",
    "        D2 = Delta2/m + _lambda/m * np.c_[np.zeros((Theta2.shape[0])), Theta2[:,1:]]\n",
    "        D1 = Delta1/m + _lambda/m * np.c_[np.zeros((Theta1.shape[0])), Theta1[:,1:]]\n",
    "        return np.r_[D1.ravel(order='F'), D2.ravel(order='F')]\n",
    "\n",
    "    \n",
    "def predict_from_three_layer_NN(Theta1, Theta2, X):\n",
    "    m, _ = X.shape\n",
    "    A_1 = np.c_[np.ones((m)), X] # (5000, 400)\n",
    "    \n",
    "    Z_2 = Theta1.dot(A_1.T) # (25, 401) * (401, 5000)\n",
    "    A_tmp = sigmoid(Z_2).T # (5000, 25)    \n",
    "    A_2 = np.c_[(np.ones((m)), A_tmp)] # (5000, 26) \n",
    "    \n",
    "    Z_3 = Theta2.dot(A_2.T) # (10, 26) * (26, 5000) \n",
    "    A_3 = sigmoid(Z_3).T # (5000, 10)\n",
    "    \n",
    "    return A_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer() #min_df=.0001, max_df=0.8, max_features=10000, ngram_range=(1,2)\n",
    "tfidf_vect.fit(sentences)\n",
    "X_train = tfidf_vect.transform(dataset_train)\n",
    "X_test = tfidf_vect.transform(dataset_test)\n",
    "\n",
    "input_layer_size =  X_train.shape[1]\n",
    "hidden_layer_size = 10\n",
    "num_labels = 5\n",
    "\n",
    "labels=[ 'MSA', 'LAV', 'EGY', 'GLF', 'NOR']\n",
    "org_y = np.array([labels.index(d) for d in label_train])\n",
    "tmp = org_y.copy()\n",
    "y = np.zeros((tmp.size, num_labels))\n",
    "y[np.arange(tmp.size), tmp.ravel()] = 1  # (5000, 10)\n",
    "\n",
    "# input_layer_size  = 400;  # 20x20 Input Images of Digits\n",
    "# hidden_layer_size = 10;   # 25 hidden units\n",
    "# num_labels = 10;          # 10 labels, from 1 to 10\n",
    "\n",
    "# handwritten_digits = loadmat('/home/disooqi/ml/machine-learning-ex4/ex4/ex4data1.mat')\n",
    "# handwritten_digits.keys()\n",
    "\n",
    "# X_train = handwritten_digits['X']\n",
    "# #m, n = features.shape\n",
    "\n",
    "# org_y = handwritten_digits['y']\n",
    "# tmp = org_y.copy()\n",
    "#tmp[tmp==10] = 0\n",
    "# y = np.zeros((tmp.size, num_labels))\n",
    "# y[np.arange(tmp.size), tmp.ravel()] = 1  # (5000, 10)\n",
    "\n",
    "X_train.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_lambda = 1\n",
    "\n",
    "\n",
    "# print X_train.shape\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
    "# print initial_Theta1.shape, initial_Theta2.shape\n",
    "\n",
    "initial_weights = np.r_[initial_Theta1.ravel(order='F'), initial_Theta2.ravel(order='F')]\n",
    "# print initial_weights.shape\n",
    "\n",
    "\n",
    "#D = nn_gradient(initial_weights, input_layer_size, hidden_layer_size, num_labels, X_train, y, _lambda)\n",
    "\n",
    "initial_weights = np.r_[initial_Theta1.ravel(order='F'), initial_Theta2.ravel(order='F')]\n",
    "res = minimize(fun=nnCostFunction, x0 =initial_weights, \n",
    "               args=(input_layer_size, hidden_layer_size, num_labels,X_train.toarray(), y, _lambda), method='CG', \n",
    "               jac=nn_gradient, options={'maxiter':200})\n",
    "\n",
    "theta1_size = (input_layer_size+1) * hidden_layer_size\n",
    "opt_Theta1 = res.x[:theta1_size].reshape((hidden_layer_size,input_layer_size+1), order='F') # (25, 401)\n",
    "opt_Theta2 = res.x[theta1_size:].reshape((num_labels, hidden_layer_size+1), order='F') # (10, 26)\n",
    "pred = predict_from_three_layer_NN(opt_Theta1, opt_Theta2, X_train.toarray())\n",
    "np.mean(pred.argmax(axis=1) == tmp.ravel())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = predict_from_three_layer_NN(opt_Theta1, opt_Theta2, X_test.toarray())\n",
    "\n",
    "y_test = np.array([labels.index(d) for d in label_test])\n",
    "np.mean(pred.argmax(axis=1) == y_test.ravel())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lambda = 3 and full features\n",
    "pred = predict_from_three_layer_NN(opt_Theta1, opt_Theta2, X_test.toarray())\n",
    "\n",
    "y_test = np.array([labels.index(d) for d in label_test])\n",
    "np.mean(pred.argmax(axis=1) == y_test.ravel())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = predict_from_three_layer_NN(opt_Theta1, opt_Theta2, X_train.toarray())\n",
    "np.mean(pred.argmax(axis=1) == tmp.ravel())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnb_clf = MultinomialNB()\n",
    "lr_clf = LogisticRegression()\n",
    "sgd_clf = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)\n",
    "svm_clf = svm.LinearSVC()\n",
    "\n",
    "# help (SGDClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ed9f5daf039b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mlda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearDiscriminantAnalysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m# svd = TruncatedSVD(n_components=640, random_state=42)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# u,s,v = svd.fit(X_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/disooqi/anaconda2/lib/python2.7/site-packages/scipy/sparse/compressed.pyc\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m         \u001b[1;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m     \u001b[1;31m##############################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/disooqi/anaconda2/lib/python2.7/site-packages/scipy/sparse/coo.pyc\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m         \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[0mfortran\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfortran\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/disooqi/anaconda2/lib/python2.7/site-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    815\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 817\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__numpy_ufunc__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer= 'char',lowercase=False, max_df=0.95,ngram_range=(2,5), smooth_idf=False,\n",
    "                             sublinear_tf=True)\n",
    "\n",
    "vectorizer.fit(sentences)\n",
    "X_train = vectorizer.transform(dataset_train)\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "lda.fit(X_train.toarray(), label_train)\n",
    "# svd = TruncatedSVD(n_components=640, random_state=42)\n",
    "# u,s,v = svd.fit(X_train) \n",
    "# train_pred = lda.predict(X_train)\n",
    "# test_pred = lda.predict(X_test)\n",
    "\n",
    "# print 'Training Acc: ',np.around(np.mean(pred_train == label_train)*100,2), '%'\n",
    "# print 'Testing Acc: ',np.around(np.mean(pred_test == label_test)*100,2), '%'\n",
    "print type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer= 'char',lowercase=False, max_df=0.95,ngram_range=(2,5), smooth_idf=False,\n",
    "                             sublinear_tf=True)\n",
    "\n",
    "pipeline_01 = Pipeline([('v_01', vectorizer),\n",
    "                        ('clf_01', SGDClassifier())])\n",
    "# lowercase=False, ngram_range=(1,2)\n",
    "#min_df=.0001, max_df=0.8, max_features=10000, ngram_range=(1,2)\n",
    "parameters = {\n",
    "#      'v_01__ngram_range': [(1, 1), (1, 2),(1, 3),(2,5),(3,5)],\n",
    "#     'v_01__analyzer':('word', 'char'),\n",
    "#     'v_01__use_idf': (True, False),\n",
    "#     'v_01__max_df' : np.round(np.linspace(.6,.7,10), 2),\n",
    "#     'v_01__max_df' : (0.5,.6,.66,.7, 0.95),\n",
    "#     'v_01__min_df':(1,2,3,4,5,6,7,8,9,10),\n",
    "#     'v_01__lowercase' : (True, False),\n",
    "#     'v_01__norm' : ('l1', 'l2', None),\n",
    "#     'v_01__binary': (False,True),\n",
    "#     'clf_01__alpha': (1e-2, 1e-3),\n",
    "#     'v_01__smooth_idf':(True,False),\n",
    "#     'v_01__sublinear_tf':(True,False),\n",
    "    #'clf_01__penalty':('none', 'l2', 'l1', 'elasticnet'),\n",
    "#     'clf_01__loss':('hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_loss', 'huber', \n",
    "#      'epsilon_insensitive', 'squared_epsilon_insensitive'),\n",
    "}\n",
    "gs_clf = GridSearchCV(pipeline_01, parameters, n_jobs=-1)\n",
    "gs_clf.fit(dataset_train, label_train)\n",
    "\n",
    "\n",
    "pred_train = gs_clf.predict(dataset_train)\n",
    "pred_test = gs_clf.predict(dataset_test)\n",
    "\n",
    "# pipeline_01.fit(dataset_train, label_train)\n",
    "\n",
    "# pred_train = pipeline_01.predict(dataset_train)\n",
    "# pred_test = pipeline_01.predict(dataset_test)\n",
    "\n",
    "print 'Training Acc: ',np.around(np.mean(pred_train == label_train)*100,2), '%'\n",
    "print 'Testing Acc: ',np.around(np.mean(pred_test == label_test)*100,2), '%'\n",
    "\n",
    "print(metrics.classification_report(label_test, pred_test, target_names=['LAV', 'MSA', 'EGY', 'GLF', 'NOR']))\n",
    "\n",
    "best_parameters, score, _ = max(gs_clf.grid_scores_, key=lambda x: x[1])\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Acc:  98.49 %\n",
      "Testing Acc:  65.94 %\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer= 'char',lowercase=False, max_df=0.95,ngram_range=(2,5), smooth_idf=False,\n",
    "                             sublinear_tf=True)\n",
    "\n",
    "vectorizer.fit(sentences)\n",
    "X_train = vectorizer.transform(dataset_train)\n",
    "X_test = vectorizer.transform(dataset_test)\n",
    "# X_removed = vectorizer.transform(removed_sentences)\n",
    "\n",
    "sgd_clf_02 = SGDClassifier()\n",
    "sgd_clf_02.fit(X_train, label_train)\n",
    "\n",
    "pred_train = sgd_clf_02.predict(X_train)\n",
    "pred_test = sgd_clf_02.predict(X_test)\n",
    "# pred_removed = sgd_clf_02.predict(X_removed)\n",
    "\n",
    "print 'Training Acc: ',np.around(np.mean(pred_train == label_train)*100,2), '%'\n",
    "print 'Testing Acc: ',np.around(np.mean(pred_test == label_test)*100,2), '%'\n",
    "# print 'removed Acc: ',np.around(np.mean(pred_removed == removed_sentences_labels)*100,2), '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_clf.fit(X_train, label_train)\n",
    "y_pred = lr_clf.predict(X_test)                  # .score(X_train, label_train)\n",
    "\n",
    "\n",
    "print 'Training Acc: ',np.around(lr_clf.score(X_train, label_train)*100,2), '%'\n",
    "print 'Testing Acc: ',np.around(lr_clf.score(X_test, label_test)*100,2), '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd_clf.fit(X_train, label_train)\n",
    "train_pred = sgd_clf.predict(X_train) \n",
    "y_pred = sgd_clf.predict(X_test) \n",
    "\n",
    "\n",
    "print 'Training Acc: ',np.around(np.mean(train_pred == label_train)*100,2), '%'\n",
    "print 'Testing Acc: ',np.around(np.mean(y_pred == label_test)*100,2), '%'\n",
    "\n",
    "print(metrics.classification_report(label_test, y_pred, target_names=['LAV', 'MSA', 'EGY', 'GLF', 'NOR']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(label_test, y_pred, labels=[ 'MSA', 'LAV', 'EGY', 'GLF', 'NOR'])\n",
    "cm_normalized = np.round(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] *100,3)\n",
    "\n",
    "print cm_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = X_test.toarray()\n",
    "\n",
    "dd = y_pred!=label_test\n",
    "for i in np.arange(dd.size):\n",
    "    if dd[i]:\n",
    "        \n",
    "        if label_test[i]=='LAV' and y_pred[i]=='EGY':\n",
    "            print from_buck_to_utf8(dataset_test[i]), label_test[i], y_pred[i]\n",
    "        print '-' * 80\n",
    "        \n",
    "\n",
    "#dataset_test\n",
    "#A[y_pred != label_test].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "['زين', 'ما صار', 'اللي', 'شو', 'لحتى', 'زي ما بيقولوا', 'مش هيجي', 'هون','ما بينفعنا','','','','','','','','','']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

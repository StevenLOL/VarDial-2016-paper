{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import codecs\n",
    "from collections import Counter\n",
    "import random \n",
    "import numpy as np\n",
    "from numpy.random import permutation, shuffle, rand\n",
    "from numpy.linalg import svd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_path = r'/home/disooqi/qcri/dialects/task/DSL-training/task2-train.txt'\n",
    "test_set_path = r'/home/disooqi/qcri/dialects/task/DSL2016-test/C.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def from_buck_to_utf8(text):\n",
    "    b2a = {'A': u'\\u0627',  '<': u'\\u0625',  '|': u'\\u0622',  '>': u'\\u0623',  \"'\": u'\\u0621',  'b': u'\\u0628',  \n",
    "           't': u'\\u062a',  'v': u'\\u062b',  'j': u'\\u062c',  'H': u'\\u062d',  'x': u'\\u062e',  'd': u'\\u062f',  \n",
    "           '*': u'\\u0630',  'r': u'\\u0631',  'z': u'\\u0632',  's': u'\\u0633',  '$': u'\\u0634',  'S': u'\\u0635',  \n",
    "           'D': u'\\u0636',  'T': u'\\u0637',  'Z': u'\\u0638',  'E': u'\\u0639',  'g': u'\\u063a',  'f': u'\\u0641',  \n",
    "           'q': u'\\u0642',  'k': u'\\u0643',  'l': u'\\u0644',  'm': u'\\u0645',  'n': u'\\u0646',  'h': u'\\u0647',  \n",
    "           'w': u'\\u0648',  'y': u'\\u064a',  'Y': u'\\u0649',  'p': u'\\u0629',  '&': u'\\u0624',  '}': u'\\u0626',  \n",
    "           'a': u'\\u064e',  'F': u'\\u064b',  'u': u'\\u064f',  'N': u'\\u064c',  'i': u'\\u0650',  'K': u'\\u064d',  \n",
    "           'o': u'\\u0652',  '~': u'\\u0651'}\n",
    "    text = text.strip().split()\n",
    "    tmp_sentence = list()\n",
    "    for word in text:\n",
    "        tmp_word = list()\n",
    "        for c in word:\n",
    "            tmp_word.append(b2a.get(c,c))\n",
    "        else:\n",
    "            tmp_sentence.append(''.join(tmp_word))\n",
    "    else:\n",
    "        return ' '.join(tmp_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sentence count: 1540\n"
     ]
    }
   ],
   "source": [
    "test_sentences2 = list()\n",
    "with codecs.open(test_set_path) as testf:\n",
    "    for i, line in enumerate(testf):\n",
    "#         if len(line.strip().split()) <= 1:\n",
    "#             print i, from_buck_to_utf8(line.strip())\n",
    "#             continue\n",
    "        test_sentences2.append(line.strip())\n",
    "    else:\n",
    "        print 'Test sentence count:', len(test_sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count_vect = CountVectorizer()\n",
    "# X_train = count_vect.fit_transform(sentences)\n",
    "# X_test = count_vect.transform(test_sentences)\n",
    "\n",
    "# train_one_str = ' '.join(sentences)\n",
    "# test_one_str = ' '.join(test_sentences)\n",
    "\n",
    "# words_in_train = set(train_one_str.split())\n",
    "# words_in_test = set(test_one_str.split())\n",
    "# print len(words_in_test), len(words_in_train), len(count_vect.vocabulary_)\n",
    "\n",
    "# OOV = list()\n",
    "# for word in words_in_test:\n",
    "#     if count_vect.vocabulary_.get(unicode(word), 0) == 0:\n",
    "#         OOV.append(word)\n",
    "\n",
    "# print 'Out-Of-Vocab: ', len(OOV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QCRI-closed-C-run1.txt -CANDIDATE 01"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using SVM\n",
    "Using words as features\n",
    "considering all sentences for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOR', 'LAV', 'GLF', 'EGY', 'MSA']\n",
      "LAV training dataset:  1758 , cross-validation set:  0 , test: 0\n",
      "MSA training dataset:  999 , cross-validation set:  0 , test: 0\n",
      "EGY training dataset:  1578 , cross-validation set:  0 , test: 0\n",
      "GLF training dataset:  1672 , cross-validation set:  0 , test: 0\n",
      "NOR training dataset:  1612 , cross-validation set:  0 , test: 0\n",
      "----------------------------------------------------------------------\n",
      "Total  ...  Training:  7619 , cross-validation data 0 , test:  0\n"
     ]
    }
   ],
   "source": [
    "sentences = list()\n",
    "removed_sentences = list()\n",
    "removed_sentences_labels = list()\n",
    "labels = list()\n",
    "\n",
    "labels_dist = set()\n",
    "\n",
    "dataset = dict()\n",
    "#We will release training and testing data for the following Arabic dialects: \n",
    "# Egyptian, Gulf, Levantine, and North-African, and Modern Standard Arabic (MSA)\n",
    "\n",
    "with codecs.open(r'/home/disooqi/qcri/dialects/task/DSL-training/task2-train.txt') as trainingf:\n",
    "    LAV = list()\n",
    "    MSA = list()\n",
    "    EGY = list()\n",
    "    GLF = list()\n",
    "    NOR = list()\n",
    "    for i, line in enumerate(trainingf):\n",
    "        sentence_label = line.strip().split('\\t')\n",
    "        utf8_sentence = sentence_label[0]\n",
    "        \n",
    "        # labels.append(sentence_label[2])\n",
    "        \n",
    "        if len(utf8_sentence.strip().split()) <= 0:\n",
    "            removed_sentences.append(utf8_sentence)\n",
    "            removed_sentences_labels.append(sentence_label[2])\n",
    "#             print i, sentence_label[0]\n",
    "            continue\n",
    "        \n",
    "        sentences.append(utf8_sentence)\n",
    "        if sentence_label[2] == 'LAV':\n",
    "            LAV.append(utf8_sentence)\n",
    "        elif sentence_label[2] == 'MSA':\n",
    "            MSA.append(utf8_sentence)\n",
    "        elif sentence_label[2] == 'EGY':\n",
    "            EGY.append(utf8_sentence)\n",
    "        elif sentence_label[2] == 'GLF':\n",
    "            GLF.append(utf8_sentence)\n",
    "        elif sentence_label[2] == 'NOR':\n",
    "            NOR.append(utf8_sentence)\n",
    "        else:\n",
    "            print(utf8_sentence)\n",
    "    else:\n",
    "#         print 'sentence count:', len(sentences)\n",
    "#         print set(labels)\n",
    "        dataset['LAV'] = LAV\n",
    "        dataset['MSA'] = MSA\n",
    "        dataset['EGY'] = EGY\n",
    "        dataset['GLF'] = GLF\n",
    "        dataset['NOR'] = NOR\n",
    "        LAV = list()\n",
    "        MSA = list()\n",
    "        EGY = list()\n",
    "        GLF = list()\n",
    "        NOR = list()\n",
    "\n",
    "target_names = dataset.keys()\n",
    "print target_names\n",
    "\n",
    "def divide_dataset(dataset ,CV=True, train_perc=80 , CV_perc=0, test_perc=20):\n",
    "    if train_perc + CV_perc + test_perc != 100:\n",
    "        print 'the sum of percs is not 100'\n",
    "        return\n",
    "    samples_train = dict()\n",
    "    samples_cv = dict()\n",
    "    samples_test = dict()\n",
    "    \n",
    "    for dialect, sentences in dataset.items():\n",
    "        samples = permutation(sentences)\n",
    "        train_len = int(np.ceil(len(samples)*(train_perc/100.0)))\n",
    "        samples_train[dialect] = sentences[:train_len]\n",
    "        cv_len = 0\n",
    "        if CV:\n",
    "            cvp = CV_perc/(100.0-60)\n",
    "            cv_len = int(np.ceil((len(samples)-train_len) * cvp))\n",
    "            samples_cv[dialect] = sentences[train_len:train_len+cv_len]\n",
    "            samples_test[dialect] = sentences[train_len+cv_len:]\n",
    "        else:\n",
    "            samples_cv[dialect] = list()\n",
    "            samples_test[dialect] = sentences[train_len:]\n",
    "    else:\n",
    "        return samples_train, samples_cv, samples_test\n",
    "            \n",
    "\n",
    "train_set, cv_set, test_set = divide_dataset(dataset, CV=False, train_perc=100 ,CV_perc=0, test_perc=0)\n",
    "\n",
    "target_names = ['LAV', 'MSA', 'EGY', 'GLF', 'NOR']\n",
    "\n",
    "t,c,ts = 0,0,0\n",
    "for dial in ['LAV', 'MSA', 'EGY', 'GLF', 'NOR']:\n",
    "    t += len(train_set[dial])\n",
    "    c += len(cv_set[dial])\n",
    "    ts+= len(test_set[dial])\n",
    "    print dial, 'training dataset: ', len(train_set[dial]), ', cross-validation set: ', \\\n",
    "    len(cv_set[dial]),', test:', len(test_set[dial])\n",
    "    \n",
    "else:\n",
    "    print 70*'-'\n",
    "    print 'Total  ...  Training: ', t, ', cross-validation data', c, ', test: ', ts\n",
    "\n",
    "dataset_train = train_set['LAV']+train_set['MSA']+train_set['EGY']+train_set['GLF']+train_set['NOR']\n",
    "dataset_cv = cv_set['LAV']+cv_set['MSA']+cv_set['EGY']+cv_set['GLF']+cv_set['NOR']\n",
    "dataset_test = test_set['LAV']+test_set['MSA']+test_set['EGY']+test_set['GLF']+test_set['NOR']\n",
    "\n",
    "\n",
    "label_train = ['LAV' for x in train_set['LAV']] + ['MSA' for x in train_set['MSA']] +\\\n",
    "['EGY' for x in train_set['EGY']] + ['GLF' for x in train_set['GLF']]+['NOR' for x in train_set['NOR']]\n",
    "\n",
    "label_cv = ['LAV' for x in cv_set['LAV']] + ['MSA' for x in cv_set['MSA']] +\\\n",
    "['EGY' for x in cv_set['EGY']] + ['GLF' for x in cv_set['GLF']]+['NOR' for x in cv_set['NOR']]\n",
    "\n",
    "label_test = ['LAV' for x in test_set['LAV']] + ['MSA' for x in test_set['MSA']] +\\\n",
    "['EGY' for x in test_set['EGY']] + ['GLF' for x in test_set['GLF']]+['NOR' for x in test_set['NOR']]\n",
    "\n",
    "train_set, cv_set, test_set = 0,0,0\n",
    "#print len(label_train),len(label_cv),len(label_test)\n",
    "\n",
    "train_zipped = zip(dataset_train, label_train)\n",
    "random.shuffle(train_zipped)\n",
    "dataset_train, label_train = zip(*train_zipped)\n",
    "\n",
    "if dataset_cv:\n",
    "    cv_zipped = zip(dataset_cv, label_cv)\n",
    "    random.shuffle(cv_zipped)\n",
    "    dataset_cv, label_cv = zip(*cv_zipped)\n",
    "\n",
    "if dataset_test:\n",
    "    dataset_test.extend(removed_sentences)\n",
    "    label_test.extend(removed_sentences_labels)\n",
    "    test_zipped = zip(dataset_test, label_test)\n",
    "    random.shuffle(test_zipped)\n",
    "    dataset_test, label_test = zip(*test_zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Acc:  98.25 %\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer= 'char',lowercase=False, max_df=0.95,ngram_range=(2,5), smooth_idf=False,\n",
    "                             sublinear_tf=True)\n",
    "\n",
    "vectorizer.fit(sentences)\n",
    "X_train = vectorizer.transform(dataset_train)\n",
    "X_test = vectorizer.transform(test_sentences2)\n",
    "\n",
    "\n",
    "sgd_clf_02 = SGDClassifier()\n",
    "sgd_clf_02.fit(X_train, label_train)\n",
    "\n",
    "pred_train = sgd_clf_02.predict(X_train)\n",
    "pred_test = sgd_clf_02.predict(X_test)\n",
    "\n",
    "\n",
    "with codecs.open('QCRI-closed-C-run1.txt', mode='w', encoding='utf8') as outfile:\n",
    "    if len(test_sentences2) != len(pred_test):\n",
    "        print 'something wrong'\n",
    "    else:\n",
    "        for ts, tl in zip(test_sentences2, pred_test):\n",
    "            outfile.write(ts)\n",
    "            outfile.write('\\t')\n",
    "            outfile.write(tl)\n",
    "            outfile.write('\\n')\n",
    "            \n",
    "\n",
    "print 'Training Acc: ',np.around(np.mean(pred_train == label_train)*100,2), '%'\n",
    "# print 'Testing Acc: ',np.around(np.mean(pred_test == label_test)*100,2), '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# QCRI-closed-C-run2.txt -CANDIDATE 01"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Same as previous (C-01) but neglect sentences less than 3 words in training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOR', 'LAV', 'GLF', 'EGY', 'MSA']\n",
      "LAV training dataset:  1575 , cross-validation set:  0 , test: 0\n",
      "MSA training dataset:  990 , cross-validation set:  0 , test: 0\n",
      "EGY training dataset:  1514 , cross-validation set:  0 , test: 0\n",
      "GLF training dataset:  1479 , cross-validation set:  0 , test: 0\n",
      "NOR training dataset:  1401 , cross-validation set:  0 , test: 0\n",
      "----------------------------------------------------------------------\n",
      "Total  ...  Training:  6959 , cross-validation data 0 , test:  0\n"
     ]
    }
   ],
   "source": [
    "sentences = list()\n",
    "removed_sentences = list()\n",
    "removed_sentences_labels = list()\n",
    "labels = list()\n",
    "\n",
    "labels_dist = set()\n",
    "\n",
    "dataset = dict()\n",
    "#We will release training and testing data for the following Arabic dialects: \n",
    "# Egyptian, Gulf, Levantine, and North-African, and Modern Standard Arabic (MSA)\n",
    "\n",
    "with codecs.open(r'/home/disooqi/qcri/dialects/task/DSL-training/task2-train.txt') as trainingf:\n",
    "    LAV = list()\n",
    "    MSA = list()\n",
    "    EGY = list()\n",
    "    GLF = list()\n",
    "    NOR = list()\n",
    "    for i, line in enumerate(trainingf):\n",
    "        sentence_label = line.strip().split('\\t')\n",
    "        utf8_sentence = sentence_label[0]\n",
    "        \n",
    "        # labels.append(sentence_label[2])\n",
    "        \n",
    "        if len(utf8_sentence.strip().split()) <= 3:\n",
    "            removed_sentences.append(utf8_sentence)\n",
    "            removed_sentences_labels.append(sentence_label[2])\n",
    "#             print i, sentence_label[0]\n",
    "            continue\n",
    "        \n",
    "        sentences.append(utf8_sentence)\n",
    "        if sentence_label[2] == 'LAV':\n",
    "            LAV.append(utf8_sentence)\n",
    "        elif sentence_label[2] == 'MSA':\n",
    "            MSA.append(utf8_sentence)\n",
    "        elif sentence_label[2] == 'EGY':\n",
    "            EGY.append(utf8_sentence)\n",
    "        elif sentence_label[2] == 'GLF':\n",
    "            GLF.append(utf8_sentence)\n",
    "        elif sentence_label[2] == 'NOR':\n",
    "            NOR.append(utf8_sentence)\n",
    "        else:\n",
    "            print(utf8_sentence)\n",
    "    else:\n",
    "#         print 'sentence count:', len(sentences)\n",
    "#         print set(labels)\n",
    "        dataset['LAV'] = LAV\n",
    "        dataset['MSA'] = MSA\n",
    "        dataset['EGY'] = EGY\n",
    "        dataset['GLF'] = GLF\n",
    "        dataset['NOR'] = NOR\n",
    "        LAV = list()\n",
    "        MSA = list()\n",
    "        EGY = list()\n",
    "        GLF = list()\n",
    "        NOR = list()\n",
    "\n",
    "target_names = dataset.keys()\n",
    "print target_names\n",
    "\n",
    "def divide_dataset(dataset ,CV=True, train_perc=80 , CV_perc=0, test_perc=20):\n",
    "    if train_perc + CV_perc + test_perc != 100:\n",
    "        print 'the sum of percs is not 100'\n",
    "        return\n",
    "    samples_train = dict()\n",
    "    samples_cv = dict()\n",
    "    samples_test = dict()\n",
    "    \n",
    "    for dialect, sentences in dataset.items():\n",
    "        samples = permutation(sentences)\n",
    "        train_len = int(np.ceil(len(samples)*(train_perc/100.0)))\n",
    "        samples_train[dialect] = sentences[:train_len]\n",
    "        cv_len = 0\n",
    "        if CV:\n",
    "            cvp = CV_perc/(100.0-60)\n",
    "            cv_len = int(np.ceil((len(samples)-train_len) * cvp))\n",
    "            samples_cv[dialect] = sentences[train_len:train_len+cv_len]\n",
    "            samples_test[dialect] = sentences[train_len+cv_len:]\n",
    "        else:\n",
    "            samples_cv[dialect] = list()\n",
    "            samples_test[dialect] = sentences[train_len:]\n",
    "    else:\n",
    "        return samples_train, samples_cv, samples_test\n",
    "            \n",
    "\n",
    "train_set, cv_set, test_set = divide_dataset(dataset, CV=False, train_perc=100 ,CV_perc=0, test_perc=0)\n",
    "\n",
    "target_names = ['LAV', 'MSA', 'EGY', 'GLF', 'NOR']\n",
    "\n",
    "t,c,ts = 0,0,0\n",
    "for dial in ['LAV', 'MSA', 'EGY', 'GLF', 'NOR']:\n",
    "    t += len(train_set[dial])\n",
    "    c += len(cv_set[dial])\n",
    "    ts+= len(test_set[dial])\n",
    "    print dial, 'training dataset: ', len(train_set[dial]), ', cross-validation set: ', \\\n",
    "    len(cv_set[dial]),', test:', len(test_set[dial])\n",
    "    \n",
    "else:\n",
    "    print 70*'-'\n",
    "    print 'Total  ...  Training: ', t, ', cross-validation data', c, ', test: ', ts\n",
    "\n",
    "dataset_train = train_set['LAV']+train_set['MSA']+train_set['EGY']+train_set['GLF']+train_set['NOR']\n",
    "dataset_cv = cv_set['LAV']+cv_set['MSA']+cv_set['EGY']+cv_set['GLF']+cv_set['NOR']\n",
    "dataset_test = test_set['LAV']+test_set['MSA']+test_set['EGY']+test_set['GLF']+test_set['NOR']\n",
    "\n",
    "\n",
    "label_train = ['LAV' for x in train_set['LAV']] + ['MSA' for x in train_set['MSA']] +\\\n",
    "['EGY' for x in train_set['EGY']] + ['GLF' for x in train_set['GLF']]+['NOR' for x in train_set['NOR']]\n",
    "\n",
    "label_cv = ['LAV' for x in cv_set['LAV']] + ['MSA' for x in cv_set['MSA']] +\\\n",
    "['EGY' for x in cv_set['EGY']] + ['GLF' for x in cv_set['GLF']]+['NOR' for x in cv_set['NOR']]\n",
    "\n",
    "label_test = ['LAV' for x in test_set['LAV']] + ['MSA' for x in test_set['MSA']] +\\\n",
    "['EGY' for x in test_set['EGY']] + ['GLF' for x in test_set['GLF']]+['NOR' for x in test_set['NOR']]\n",
    "\n",
    "train_set, cv_set, test_set = 0,0,0\n",
    "#print len(label_train),len(label_cv),len(label_test)\n",
    "\n",
    "train_zipped = zip(dataset_train, label_train)\n",
    "random.shuffle(train_zipped)\n",
    "dataset_train, label_train = zip(*train_zipped)\n",
    "\n",
    "if dataset_cv:\n",
    "    cv_zipped = zip(dataset_cv, label_cv)\n",
    "    random.shuffle(cv_zipped)\n",
    "    dataset_cv, label_cv = zip(*cv_zipped)\n",
    "\n",
    "if dataset_test:\n",
    "    dataset_test.extend(removed_sentences)\n",
    "    label_test.extend(removed_sentences_labels)\n",
    "    test_zipped = zip(dataset_test, label_test)\n",
    "    random.shuffle(test_zipped)\n",
    "    dataset_test, label_test = zip(*test_zipped)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Acc:  99.64 %\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer= 'char',lowercase=False, max_df=0.95,ngram_range=(2,5), smooth_idf=False,\n",
    "                             sublinear_tf=True)\n",
    "\n",
    "vectorizer.fit(sentences)\n",
    "X_train = vectorizer.transform(dataset_train)\n",
    "X_test = vectorizer.transform(test_sentences2)\n",
    "\n",
    "\n",
    "sgd_clf_02 = SGDClassifier()\n",
    "sgd_clf_02.fit(X_train, label_train)\n",
    "\n",
    "pred_train = sgd_clf_02.predict(X_train)\n",
    "pred_test = sgd_clf_02.predict(X_test)\n",
    "\n",
    "with codecs.open('QCRI-closed-C-run2.txt', mode='w', encoding='utf8') as outfile:\n",
    "    if len(test_sentences2) != len(pred_test):\n",
    "        print 'something wrong'\n",
    "    else:\n",
    "        for ts, tl in zip(test_sentences2, pred_test):\n",
    "            outfile.write(ts)\n",
    "            outfile.write('\\t')\n",
    "            outfile.write(tl)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "print 'Training Acc: ',np.around(np.mean(pred_train == label_train)*100,2), '%'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

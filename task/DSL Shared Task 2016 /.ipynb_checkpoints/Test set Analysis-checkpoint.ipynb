{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import codecs\n",
    "from collections import Counter\n",
    "import random \n",
    "import numpy as np\n",
    "from numpy.random import permutation, shuffle, rand\n",
    "from numpy.linalg import svd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = r'/home/disooqi/qcri/dialects/task/DSL-training/task2-train.txt'\n",
    "test_set = r'/home/disooqi/qcri/dialects/task/DSL2016-test/C.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def from_buck_to_utf8(text):\n",
    "    b2a = {'A': u'\\u0627',  '<': u'\\u0625',  '|': u'\\u0622',  '>': u'\\u0623',  \"'\": u'\\u0621',  'b': u'\\u0628',  \n",
    "           't': u'\\u062a',  'v': u'\\u062b',  'j': u'\\u062c',  'H': u'\\u062d',  'x': u'\\u062e',  'd': u'\\u062f',  \n",
    "           '*': u'\\u0630',  'r': u'\\u0631',  'z': u'\\u0632',  's': u'\\u0633',  '$': u'\\u0634',  'S': u'\\u0635',  \n",
    "           'D': u'\\u0636',  'T': u'\\u0637',  'Z': u'\\u0638',  'E': u'\\u0639',  'g': u'\\u063a',  'f': u'\\u0641',  \n",
    "           'q': u'\\u0642',  'k': u'\\u0643',  'l': u'\\u0644',  'm': u'\\u0645',  'n': u'\\u0646',  'h': u'\\u0647',  \n",
    "           'w': u'\\u0648',  'y': u'\\u064a',  'Y': u'\\u0649',  'p': u'\\u0629',  '&': u'\\u0624',  '}': u'\\u0626',  \n",
    "           'a': u'\\u064e',  'F': u'\\u064b',  'u': u'\\u064f',  'N': u'\\u064c',  'i': u'\\u0650',  'K': u'\\u064d',  \n",
    "           'o': u'\\u0652',  '~': u'\\u0651'}\n",
    "    text = text.strip().split()\n",
    "    tmp_sentence = list()\n",
    "    for word in text:\n",
    "        tmp_word = list()\n",
    "        for c in word:\n",
    "            tmp_word.append(b2a.get(c,c))\n",
    "        else:\n",
    "            tmp_sentence.append(''.join(tmp_word))\n",
    "    else:\n",
    "        return ' '.join(tmp_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sentence count: 7619\n",
      "135 أميركا\n",
      "166 أقسام\n",
      "179 أسعار\n",
      "365 الحقيقة\n",
      "617 بحث\n",
      "704 على\n",
      "944 كتب\n",
      "945 كتب\n",
      "984 لبنان\n",
      "985 لبنان\n",
      "1052 ما\n",
      "1158 نعم\n",
      "1263 تعليم\n",
      "Test sentence count: 1527\n"
     ]
    }
   ],
   "source": [
    "test_sentences = list()\n",
    "with codecs.open(test_set) as test:\n",
    "    for i, line in enumerate(test):\n",
    "#         if len(line.strip().split()) <= 1:\n",
    "#             print i, from_buck_to_utf8(line.strip())\n",
    "#             continue\n",
    "        test_sentences.append(from_buck_to_utf8(line.strip()))\n",
    "    else:\n",
    "        print 'Test sentence count:', len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train = count_vect.fit_transform(sentences)\n",
    "X_test = count_vect.transform(test_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19493 55992 55929\n",
      "Out-Of-Vocab:  6803\n"
     ]
    }
   ],
   "source": [
    "train_one_str = ' '.join(sentences)\n",
    "test_one_str = ' '.join(test_sentences)\n",
    "\n",
    "words_in_train = set(train_one_str.split())\n",
    "words_in_test = set(test_one_str.split())\n",
    "print len(words_in_test), len(words_in_train), len(count_vect.vocabulary_)\n",
    "\n",
    "OOV = list()\n",
    "for word in words_in_test:\n",
    "    if count_vect.vocabulary_.get(unicode(word), 0) == 0:\n",
    "        OOV.append(word)\n",
    "\n",
    "print 'Out-Of-Vocab: ', len(OOV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QCRI-closed-C-run1.txt -CANDIDATE 01"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = list()\n",
    "removed_sentences = list()\n",
    "removed_sentences_labels = list()\n",
    "labels = list()\n",
    "\n",
    "labels_dist = set()\n",
    "\n",
    "dataset = dict()\n",
    "#We will release training and testing data for the following Arabic dialects: \n",
    "# Egyptian, Gulf, Levantine, and North-African, and Modern Standard Arabic (MSA)\n",
    "\n",
    "with codecs.open('/home/disooqi/qcri/dialects/task/DSL-training/task2-train.txt') as training:\n",
    "    LAV = list()\n",
    "    MSA = list()\n",
    "    EGY = list()\n",
    "    GLF = list()\n",
    "    NOR = list()\n",
    "    for i, line in enumerate(training):\n",
    "        sentence_label = line.strip().split('\\t')\n",
    "        utf8_sentence = from_buck_to_utf8(sentence_label[0])\n",
    "        \n",
    "        # labels.append(sentence_label[2])\n",
    "        \n",
    "        if len(utf8_sentence.strip().split()) <= 0:\n",
    "            removed_sentences.append(utf8_sentence)\n",
    "            removed_sentences_labels.append(sentence_label[2])\n",
    "#             print i, sentence_label[0]\n",
    "            continue\n",
    "        \n",
    "        sentences.append(utf8_sentence)\n",
    "        if sentence_label[2] == 'LAV':\n",
    "            LAV.append(utf8_sentence)\n",
    "        elif sentence_label[2] == 'MSA':\n",
    "            MSA.append(utf8_sentence)\n",
    "        elif sentence_label[2] == 'EGY':\n",
    "            EGY.append(utf8_sentence)\n",
    "        elif sentence_label[2] == 'GLF':\n",
    "            GLF.append(utf8_sentence)\n",
    "        elif sentence_label[2] == 'NOR':\n",
    "            NOR.append(utf8_sentence)\n",
    "        else:\n",
    "            print(utf8_sentence)\n",
    "    else:\n",
    "#         print 'sentence count:', len(sentences)\n",
    "#         print set(labels)\n",
    "        dataset['LAV'] = LAV\n",
    "        dataset['MSA'] = MSA\n",
    "        dataset['EGY'] = EGY\n",
    "        dataset['GLF'] = GLF\n",
    "        dataset['NOR'] = NOR\n",
    "        LAV = list()\n",
    "        MSA = list()\n",
    "        EGY = list()\n",
    "        GLF = list()\n",
    "        NOR = list()\n",
    "\n",
    "target_names = dataset.keys()\n",
    "\n",
    "def divide_dataset(dataset ,CV=True, train_perc=80 , CV_perc=0, test_perc=20):\n",
    "    if train_perc + CV_perc + test_perc != 100:\n",
    "        print 'the sum of percs is not 100'\n",
    "        return\n",
    "    samples_train = dict()\n",
    "    samples_cv = dict()\n",
    "    samples_test = dict()\n",
    "    \n",
    "    for dialect, sentences in dataset.items():\n",
    "        samples = permutation(sentences)\n",
    "        train_len = int(np.ceil(len(samples)*(train_perc/100.0)))\n",
    "        samples_train[dialect] = sentences[:train_len]\n",
    "        cv_len = 0\n",
    "        if CV:\n",
    "            cvp = CV_perc/(100.0-60)\n",
    "            cv_len = int(np.ceil((len(samples)-train_len) * cvp))\n",
    "            samples_cv[dialect] = sentences[train_len:train_len+cv_len]\n",
    "            samples_test[dialect] = sentences[train_len+cv_len:]\n",
    "        else:\n",
    "            samples_cv[dialect] = list()\n",
    "            samples_test[dialect] = sentences[train_len:]\n",
    "    else:\n",
    "        return samples_train, samples_cv, samples_test\n",
    "            \n",
    "\n",
    "train_set, cv_set, test_set = divide_dataset(dataset, CV=False, train_perc=80 ,CV_perc=0, test_perc=20)\n",
    "\n",
    "\n",
    "t,c,ts = 0,0,0\n",
    "for dial in ['LAV', 'MSA', 'EGY', 'GLF', 'NOR']:\n",
    "    t += len(train_set[dial])\n",
    "    c += len(cv_set[dial])\n",
    "    ts+= len(test_set[dial])\n",
    "    print dial, 'training dataset: ', len(train_set[dial]), ', cross-validation set: ', \\\n",
    "    len(cv_set[dial]),', test:', len(test_set[dial])\n",
    "    \n",
    "else:\n",
    "    print 70*'-'\n",
    "    print 'Total  ...  Training: ', t, ', cross-validation data', c, ', test: ', ts\n",
    "\n",
    "dataset_train = train_set['LAV']+train_set['MSA']+train_set['EGY']+train_set['GLF']+train_set['NOR']\n",
    "dataset_cv = cv_set['LAV']+cv_set['MSA']+cv_set['EGY']+cv_set['GLF']+cv_set['NOR']\n",
    "dataset_test = test_set['LAV']+test_set['MSA']+test_set['EGY']+test_set['GLF']+test_set['NOR']\n",
    "\n",
    "\n",
    "label_train = ['LAV' for x in train_set['LAV']] + ['MSA' for x in train_set['MSA']] +\\\n",
    "['EGY' for x in train_set['EGY']] + ['GLF' for x in train_set['GLF']]+['NOR' for x in train_set['NOR']]\n",
    "\n",
    "label_cv = ['LAV' for x in cv_set['LAV']] + ['MSA' for x in cv_set['MSA']] +\\\n",
    "['EGY' for x in cv_set['EGY']] + ['GLF' for x in cv_set['GLF']]+['NOR' for x in cv_set['NOR']]\n",
    "\n",
    "label_test = ['LAV' for x in test_set['LAV']] + ['MSA' for x in test_set['MSA']] +\\\n",
    "['EGY' for x in test_set['EGY']] + ['GLF' for x in test_set['GLF']]+['NOR' for x in test_set['NOR']]\n",
    "\n",
    "train_set, cv_set, test_set = 0,0,0\n",
    "#print len(label_train),len(label_cv),len(label_test)\n",
    "\n",
    "train_zipped = zip(dataset_train, label_train)\n",
    "random.shuffle(train_zipped)\n",
    "dataset_train, label_train = zip(*train_zipped)\n",
    "\n",
    "if dataset_cv:\n",
    "    cv_zipped = zip(dataset_cv, label_cv)\n",
    "    random.shuffle(cv_zipped)\n",
    "    dataset_cv, label_cv = zip(*cv_zipped)\n",
    "\n",
    "if dataset_test:\n",
    "    dataset_test.extend(removed_sentences)\n",
    "    label_test.extend(removed_sentences_labels)\n",
    "    test_zipped = zip(dataset_test, label_test)\n",
    "    random.shuffle(test_zipped)\n",
    "    dataset_test, label_test = zip(*test_zipped)\n",
    "\n",
    "print len(dataset_test), len(removed_sentences), len(label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-478b019ecbcb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_train' is not defined"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer= 'char',lowercase=False, max_df=0.95,ngram_range=(2,5), smooth_idf=False,\n",
    "                             sublinear_tf=True)\n",
    "\n",
    "vectorizer.fit(sentences)\n",
    "X_train = vectorizer.transform(dataset_train)\n",
    "X_test = vectorizer.transform(dataset_test)\n",
    "\n",
    "sgd_clf_02 = SGDClassifier()\n",
    "sgd_clf_02.fit(X_train, label_train)\n",
    "\n",
    "pred_train = sgd_clf_02.predict(X_train)\n",
    "pred_test = sgd_clf_02.predict(X_test)\n",
    "\n",
    "# pipeline_01.fit(dataset_train, label_train)\n",
    "\n",
    "# pred_train = pipeline_01.predict(dataset_train)\n",
    "# pred_test = pipeline_01.predict(dataset_test)\n",
    "\n",
    "print 'Training Acc: ',np.around(np.mean(pred_train == label_train)*100,2), '%'\n",
    "print 'Testing Acc: ',np.around(np.mean(pred_test == label_test)*100,2), '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# QCRI-closed-C-run1.txt -CANDIDATE 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
